{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNWMQ8NNmduSDiemzhFq/Mx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DaryaTereshchenko/AdClick/blob/main/Click_text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-c1cj-SsNZGx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from pprint import pprint\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout\n",
        "from tensorflow.keras.layers import IntegerLookup, Concatenate, Flatten\n",
        "from tensorflow.python.ops.numpy_ops import np_config"
      ],
      "metadata": {
        "id": "WgQR4x1yNew8"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load data\n",
        "from urllib.request import urlretrieve\n",
        "import os\n",
        "\n",
        "def download(url, file):\n",
        "    if not os.path.isfile(file):\n",
        "        print(\"Download file... \" + file + \" ...\")\n",
        "        urlretrieve(url,file)\n",
        "        print(\"File downloaded\")\n",
        "\n",
        "# d10m_url = \"https://home.ipipan.waw.pl/sj/TIB_PAN_Adv/AdClick/D100k.tsv.gz\"\n",
        "# download(d10m_url,'D10M.tsv.gz')\n",
        "# print(\"All the files are downloaded\")\n",
        "\n",
        "d100k_url = \"https://home.ipipan.waw.pl/sj/TIB_PAN_Adv/AdClick/D100k.tsv.gz\"\n",
        "download(d100k_url,'D100k.tsv.gz')\n",
        "print(\"All the files are downloaded\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0LggWLPNgyt",
        "outputId": "16d14e26-02c7-4a66-9143-502d175fb16d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download file... D100k.tsv.gz ...\n",
            "File downloaded\n",
            "All the files are downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds = tf.data.experimental.make_csv_dataset(\"D100k.tsv.gz\",\n",
        "                                           field_delim=\"\\t\",\n",
        "                                           compression_type=\"GZIP\",\n",
        "                                           num_epochs=1,\n",
        "                                           batch_size=1024,  \n",
        "                                           label_name=\"Click\", shuffle=False)\n"
      ],
      "metadata": {
        "id": "sC3LRYiPNquM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataset_partitions_tf(ds, ds_size=10000000, train_split=0.8, val_split=0.1, test_split=0.1, shuffle=True, shuffle_size=10000):\n",
        "    assert (train_split + test_split + val_split) == 1\n",
        "    \n",
        "    if shuffle:\n",
        "        # Specify seed to always have the same split distribution between runs\n",
        "        ds = ds.shuffle(shuffle_size, seed=12)\n",
        "    \n",
        "    train_size = int(train_split * ds_size)\n",
        "    val_size = int(val_split * ds_size)\n",
        "    \n",
        "    train_ds = ds.take(train_size)    \n",
        "    val_ds = ds.skip(train_size).take(val_size)\n",
        "    test_ds = ds.skip(train_size).skip(val_size)\n",
        "    \n",
        "    return train_ds, val_ds, test_ds"
      ],
      "metadata": {
        "id": "eoS6LbcsZJjy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds, val_ds, test_ds = get_dataset_partitions_tf(ds)"
      ],
      "metadata": {
        "id": "Xqp3rnMPZWZq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = train_ds.unbatch()\n",
        "\n",
        "needed_vocab = {\"AdKeyword_tokens\": [], \"AdDescription_tokens\": [], \"Query_tokens\": []}\n",
        "\n",
        "for dic in train.as_numpy_iterator():\n",
        "  for k, v in dic[0].items():\n",
        "    if k in needed_vocab.keys():\n",
        "      needed_vocab[k].append(v)\n"
      ],
      "metadata": {
        "id": "fcekXfz2PUkz"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_to_list(parameter):\n",
        "  list_of_tokens = list(map(lambda x: x.replace(b\"|\", b\" \").split(b\" \"), parameter))\n",
        "  flatten_list = [item for subl in list_of_tokens for item in subl]\n",
        "  return np.asarray(flatten_list)\n"
      ],
      "metadata": {
        "id": "E2GGJMTNYZ3z"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keyword_tokens = transform_to_list(needed_vocab[\"AdKeyword_tokens\"])\n",
        "description_tokens = transform_to_list(needed_vocab[\"AdDescription_tokens\"])\n",
        "query_tokens = transform_to_list(needed_vocab[\"Query_tokens\"])\n"
      ],
      "metadata": {
        "id": "WKgioUzIWD_Q"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(description_tokens))\n",
        "print(len(keyword_tokens))\n",
        "print(len(query_tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXQ1nLvnYI1d",
        "outputId": "9adaa2be-8f3b-4475-b36d-28ed540762e9"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "209037\n",
            "291570\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_vocab = np.unique(np.concatenate((keyword_tokens, description_tokens, query_tokens), axis=0))"
      ],
      "metadata": {
        "id": "JP-SJ7GVpacK"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(total_vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NONkyaojZxnv",
        "outputId": "30d4cc53-a9b4-4b4b-c418-adc57a01487e"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32788"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "maxlen = 128\n",
        "max_features = 25000 # max word number"
      ],
      "metadata": {
        "id": "S7RpWTlMOuhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_on_slash(input_data):\n",
        "  return tf.strings.regex_replace(input_data, \"\\|\", \" \")"
      ],
      "metadata": {
        "id": "b3bvtjrIPHFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model1():\n",
        "  inputs = {\n",
        "      \"Position\": Input(shape=(), dtype=tf.int32),\n",
        "      \"Age\": Input(shape=(), dtype=tf.int32),\n",
        "      \"Depth\": Input(shape=(), dtype=tf.int32),\n",
        "      \"Gender\": Input(shape=(), dtype=tf.int32),\n",
        "      \"AdvertiserId\": Input(shape=(), dtype=tf.int32),\n",
        "      \"AdDescription_tokens\": Input(shape=(), dtype=tf.string),\n",
        "      \"AdTitle_tokens\": Input(shape=(), dtype=tf.string),\n",
        "      \"AdKeyword_tokens\": Input(shape=(), dtype=tf.string)}\n",
        "\n",
        "  # Integer part\n",
        "  input_age = IntegerLookup(vocabulary=[1,2,3,4,5,6], output_mode=\"one_hot\", num_oov_indices=0)(inputs[\"Age\"])\n",
        "  ci_p = IntegerLookup(vocabulary=[1,2,3], output_mode=\"one_hot\", num_oov_indices=0)(inputs[\"Position\"])\n",
        "  ci_d = IntegerLookup(vocabulary=[1,2,3], output_mode=\"one_hot\", num_oov_indices=0)(inputs[\"Depth\"])\n",
        "  ci_g = IntegerLookup(vocabulary=[0,1,2], output_mode=\"one_hot\", num_oov_indices=0)(inputs[\"Gender\"])\n",
        "  encoded = Concatenate()([ci_p, input_age, ci_d, ci_g])\n",
        "  layer = tf.keras.layers.Hashing(num_bins=128, output_mode=\"one_hot\")(inputs[\"AdvertiserId\"])\n",
        "\n",
        "  conc = tf.keras.layers.concatenate([layer, encoded])\n",
        "  num = Dropout(rate=0.3)(conc)\n",
        "  num = Dense(128)(num)\n",
        "  num = Flatten()(num)\n",
        "\n",
        "# String part\n",
        "  description = tf.keras.layers.TextVectorization(max_tokens=max_features, output_mode=\"int\", pad_to_max_tokens=maxlen, standardize=split_on_slash, output_sequence_length=maxlen, vocabulary=vocab_description)(inputs[\"AdDescription_tokens\"])\n",
        "  x = tf.keras.layers.Embedding(max_features, output_dim=20, input_length=maxlen)(description)\n",
        "  \n",
        "  title = tf.keras.layers.TextVectorization(max_tokens=max_features, output_mode=\"int\", pad_to_max_tokens=maxlen, standardize=split_on_slash, output_sequence_length=maxlen, vocabulary=vocab_title)(inputs[\"AdTitle_tokens\"])\n",
        "  y = tf.keras.layers.Embedding(max_features, output_dim=20, input_length=maxlen)(title)\n",
        "\n",
        "  keywords = tf.keras.layers.TextVectorization(max_tokens=max_features, output_mode=\"int\", pad_to_max_tokens=maxlen, standardize=split_on_slash, output_sequence_length=maxlen, vocabulary=total_vocab)(inputs[\"AdKeyword_tokens\"])\n",
        "  o = tf.keras.layers.Embedding(max_features, output_dim=20, input_length=maxlen)(keywords)\n",
        "\n",
        "  c = Concatenate()([x, y, o])\n",
        "  # x = Flatten()(x)\n",
        "  # y = Flatten()(y)\n",
        "  t = Dropout(rate=0.3)(c)\n",
        "  t = Dense(128)(t)\n",
        "  t = Flatten()(t)\n",
        "\n",
        "  z = Concatenate()([t, num])\n",
        "  z = tf.keras.layers.Dense(64, activation='relu')(z)\n",
        "  z = tf.keras.layers.Dense(1, activation='sigmoid')(z)\n",
        "  \n",
        "\n",
        "  model = Model(inputs=inputs, outputs=z)\n",
        "  model.compile(optimizer=\"Adam\", loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
        "          metrics=[\"AUC\"], run_eagerly=True)\n",
        "  return model"
      ],
      "metadata": {
        "id": "d_jQEIivVU56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = build_model1()\n",
        "model1.summary()\n",
        "tf.keras.utils.plot_model(model1)"
      ],
      "metadata": {
        "id": "XHPS_HZmPwB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1.fit(train_ds.batch(128), epochs=5, verbose=1, validation_data=test_ds.batch(128))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLNUhgirQKKR",
        "outputId": "d8251509-f66d-475e-908b-0a8ff74ea7ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py:566: UserWarning: Input dict contained keys ['DisplayURL', 'AdId', 'UserID', 'Query_tokens'] which did not match any model input. They will be ignored by the model.\n",
            "  inputs = self._flatten_to_reference_inputs(inputs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 124s 158ms/step - loss: 0.1971 - auc: 0.6023\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 122s 156ms/step - loss: 0.1699 - auc: 0.7738\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 114s 146ms/step - loss: 0.1484 - auc: 0.8491\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 115s 147ms/step - loss: 0.1365 - auc: 0.8780\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 113s 144ms/step - loss: 0.1299 - auc: 0.8912\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f553b3dff70>"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model1.predict(dt_test.batch(128).take(1))"
      ],
      "metadata": {
        "id": "G3sMzdxvdRu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load data\n",
        "from urllib.request import urlretrieve\n",
        "import os\n",
        "\n",
        "def download(url, file):\n",
        "    if not os.path.isfile(file):\n",
        "        print(\"Download file... \" + file + \" ...\")\n",
        "        urlretrieve(url,file)\n",
        "        print(\"File downloaded\")\n",
        "\n",
        "d100k_url = \"https://home.ipipan.waw.pl/sj/TIB_PAN_Adv/AdClick/D5M_test_x.tsv.gz\"\n",
        "download(d100k_url,'D5M_test_x.tsv.gz')\n",
        "print(\"All the files are downloaded\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87pN1tOwXK9M",
        "outputId": "ac74e788-47d2-45e7-9c12-6a7d98db7ec9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All the files are downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = tf.data.experimental.make_csv_dataset(\"D5M_test_x.tsv.gz\",\n",
        "                                           field_delim=\"\\t\",\n",
        "                                           compression_type=\"GZIP\",\n",
        "                                           num_epochs=1,\n",
        "                                           batch_size=128,\n",
        "                                           label_name=\"Click\",\n",
        "                                           shuffle=False)\n"
      ],
      "metadata": {
        "id": "PlijUpoFYawC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d100k_url = \"https://home.ipipan.waw.pl/sj/TIB_PAN_Adv/AdClick/D10M.tsv.gz\"\n",
        "download(d100k_url,'D10M.tsv.gz')\n",
        "print(\"All the files are downloaded\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXHurx_dv6a1",
        "outputId": "3438ff7b-fbc8-48ee-ba2e-7cd900f0a992"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All the files are downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tenm_data = tf.data.experimental.make_csv_dataset(\"D10M.tsv.gz\",\n",
        "                                           field_delim=\"\\t\",\n",
        "                                           compression_type=\"GZIP\",\n",
        "                                           num_epochs=1,\n",
        "                                           batch_size=128,\n",
        "                                           label_name=\"Click\",\n",
        "                                           shuffle=False)\n"
      ],
      "metadata": {
        "id": "JEOn56IxwC-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_size=10000000\n",
        "train_split=0.8\n",
        "test_split=0.2\n",
        "train_size = int(train_split * ds_size)\n",
        "test_size = int(test_split * ds_size)\n"
      ],
      "metadata": {
        "id": "4RC_T-mVwQgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dt = tenm_data.unbatch()"
      ],
      "metadata": {
        "id": "kqjvV8OjiNFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shuffled = dt.shuffle(100)"
      ],
      "metadata": {
        "id": "cZaFCgsRmiz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dt_train = shuffled.take(train_size) \n",
        "dt_test = shuffled.skip(train_size).take(test_size)"
      ],
      "metadata": {
        "id": "unMiMYwqiwi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model2.predict(dt_test.batch(256).take(1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpMZm5f8ZRQb",
        "outputId": "0f4f4fa3-1f3b-429c-951a-3af9d072236a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py:566: UserWarning: Input dict contained keys ['DisplayURL', 'AdId', 'AdvertiserId', 'Depth', 'Position', 'UserID', 'Gender', 'Age', 'AdKeyword_tokens', 'Query_tokens'] which did not match any model input. They will be ignored by the model.\n",
            "  inputs = self._flatten_to_reference_inputs(inputs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 154s 154s/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred"
      ],
      "metadata": {
        "id": "DUGlleUozQ2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(test_data.element_spec)"
      ],
      "metadata": {
        "id": "mm-J0jqeZbyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JtpgpOI6cjYx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}